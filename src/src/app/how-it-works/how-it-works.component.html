<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Kalam" />

<div class="how-to-page">
  <br />
  <h1>How Summit Guide is Able to Classify Mountain Images:</h1>
  <p>
    To predict what mountain you are looking at, we leverage the location and what the mountain "looks like" based on the photo. We evaluate the location and image separately and each produce a probability estimate for each of the potential mountains. The probability estimates for what the mountain "looks like" are produced by a convolutional neural network. The probability estimates for the location are computed by a sigmoid function parameterized by the distance between the photo and the peak. Then the mountain with the highest probability across both measures is displayed to the user. The diagram below shows the flow of information leading to the final prediction. 
  </p>
  <img src="assets/ProcessFlow.png" />
  <br />

  <h3>Location Probability Estimation:</h3>
  <div>
    <p>
      Function <span class="italic">f(d)</span>, shown below, produces a probability estimate based on the feasibility of the mountain being visible from a given distance. The function belongs to the family of sigmoid functions, which was chosen because, in the limit, it approaches probabilities of 0 and 1. The function coefficients were hand selected to reflect the probabilities associated with average visibility on Earth. Specifically, a distance between the photo and the peak approaching 0 miles implies a probability approaching 1 because the mountain takes up nearly your entire plane of view when you're right next to it. Conversely, since the longest sightline in North America is Mt. Sanford to Denali, 199 miles apart, a distance of > 200 miles implies a probability &lt; 0.01.
    </p>
    <p class="center" [mathjax]="sigmoidFunc"></p>
    <p>
      The distance from a peak is calculated using the coordinates of the mountain's peak and the location input by the user. The difference between the distances is calculated using the haversine formula, which considers the curvature of the Earth.
    </p>
  </div>

  <br />
  <h3>Image Probability Estimation:</h3>
  <div>
    <p>
      We use a convolutional neural network to classify which peak is in the photo. The neural network's architecture is based on best practices, many of which were inspired by resnet and alexnet. The neural network can classify 6 mountains which are shown on the classifiable mountains page.
    </p>

    <h4>Data and Overfitting:</h4>
    <div>
      <p>
        For five of the six mountains, training images were obtained from bulk downloads of Google Images. After scraping the images, we manually filtered out: photos obscured by objects or people, taken "from the peak" instead of "of the peak," drawings or other artwork of the peak, duplicate photos, and photos where the peak was not featured prominently. For Pole Mountain, the photos were collected manually on a cell phone camera. To help prevent the different collection methods from influencing the model, manual cropping and additional high resolution photos were collected of the other mountains were hand collected from the internet. In total, this resulted in 917 images belonging to 6 different mountains.
      </p>
      <p>
        917 images for 6 classes presented a major data scarcity issue for training a convolutional neural network with only approximately 150 images per class because often over a thousand images per class is desirable to train convolutional neural networks for image classification. This limited number of images makes the models substantively more prone to overfitting. To address this limitation, we artificially extended the dataset with preprocessing and maximized the utility of the data with resampling. 
      </p>
      <p>
        To preprocess the images, we applied random contrast to every image before each optimization batch and treated the amount of contrast treated as a tuning parameter. During the architecture search, we also used a custom resampling scheme that trained the same architecture multiple times to maximize the data available for training and the stability of the estimate of generalization error. In this resampling scheme, we used 90 percent of the data to train the model and 10 percent as a test set. If the model achieved greater than 50% accuracy on this training test, split the process was repeated for a new 90/10 split. We repeated this iterative procedure for five iterations with increasing thresholds of 55%, 60%, and 65%. This approach meant every model could be trained on 90% of the data for each evaluation, but the generalization performance estimate exhibited stability comparable to a 50/50 train test split. The progressive thresholds ensure the additional computation was not wasted on architectures with no promise of improvement over architectures found in manual experimentation.
      </p>
    </div>

    <br />
    <h4>Architecture:</h4>
    <div>
      <p>
        The first four layers of the network alternate convolutional layers and max pooling. The convolutional layers pass a filter with fixed dimensions over the image with coefficients that are learned during the training process. The pooling layers are a down sampling operation, and max pooling takes the maxim value from each feature map. Together, the convolutions and max pooling extract features from the raw rgb pixel values of the image.
      </p>
      <img src="assets/NeuralNetworkDiagram.png" />
      <br /><br />
      <p>
        After the feature extraction, there is a dense layer. Then the network is flattened to a 1x500 layer, followed by two additional dense layers, and finally output to a 1x6 output layer where each node corresponds to a class label. Additionally, the dense layers were all subject to a dropout rate of 45%, which is a procedure that removes the weights attached to 45% of the nodes in the layer and rescales it to produce the same prediction. These dropout layers create a regularization force on the network, preventing singular nodes from causing massive swings in predictive probabilities.
      </p>
      <p>
        After training, to get probability estimates associated with each class, the output of the final layer was passed into a SoftMax function to get values summing to 1. The entire neural network was optimized using the "Adam" optimizer with a loss function of sparse categorical cross-entropy, which is the same as categorical cross entropy but set up for our one-hot encoded labels.
      </p>
    </div>

    <br />
    <h4>Hyperparameter Optimization and Neural Architecture Search:</h4>
    <div>
      <p>
        The hyperparameters tuned were the batch size, learning rate, amount of random contrast, and random rotation applied to images. Below are the ranges of hyperparameters searched over and the final configuration:
      </p>
      <p>
        Random contrast applied to the image before feeding into the network [0-0.3] - 
        <span class="bold">0.25</span>
      </p>
      <p>
        Random Rotation of the images before feeding into the network [0, 0.1] - 
        <span class="bold">0</span>
      </p>
      <p>
        A learning rate of [0.0000001, 0.05] - 
        <span class="bold">0.0005</span>
      </p>
      <p>
        The architecture of the network was jointly tuned with the hyperparameters. Below are the architecture parameters we searched over and the final selection:
      </p>
      <p>
        The number of convolutional layers [1,2,3] - 
        <span class="bold">2</span>
      </p>
      <p>
        The size of the first filter applied at the first convolution [2x2, 3x3, ..., 22x22] - 
        <span class="bold">5x5</span>
      </p>
      <p>
        The kernels of the first convolution (each sequential kernel is doubled) [3, 6, 12] - 
        <span class="bold">6</span>
      </p>
      <p>
        The size of the first dense layer [100-1250] - 
        <span class="bold">500</span>
      </p>
      <p>
        The drop-out rate [0, 0.5] - 
        <span class="bold">0.45</span>
      </p>
      <p>
        A total of 73 architectures were evaluated using the procedure described in the Data Collection and Augmentation section.
      </p>
    </div>
  </div>
  <br />
</div>
